{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Project work in Deep Learning </center> </h1> \n",
    "<h2> <center> Song Lyrics Classification Based on Genres </center> </h2>\n",
    "\n",
    "<h3> Student: Dinno Koluh (0001034376)</h3>\n",
    "\n",
    "<h4> Introduction </h4>\n",
    "<p>\n",
    "In this project we are going classify songs into genres based on their lyrics. This is inherently a task in the area of NLP (Natural Language Processing), more specifically the <i>text classification</i> problem. In our case the text to be classified is the song lyrics and the different classes are the different genres. This task has substantial real-world usage application as the big music platforms (e.g. Spotify, Deezer, SoundCloud, Apple Music...) are exposed to this task on a daily basis. \n",
    "</p>\n",
    "\n",
    "<h4> Dataset </h4>\n",
    "\n",
    "The used dataset was obtained from Kaggle (it can be found <a url=\"https://www.kaggle.com/datasets/mateibejan/multilingual-lyrics-for-genre-classification\"> here</a>). The dataset is comprised of ~300,000 samples with the following features: artist, song title, genre, language, lyrics. During the preprocessing phase we are going reduce the dataset as it is inherently imbalanced. We will address the issues and the respective solutions in the dataset. \n",
    "\n",
    "<h4> Architecture to be used </h4>\n",
    "\n",
    "To go-to architecture for NLP tasks used to be RNN (Recurrent Neural Networks) and their modifications (LSTM, GRU) as we are dealing with inherently sequential data. RNNs are able to capture contextual information as they are able to store information from previous inputs. But this fact is also the bottleneck as RNNs have long-term dependency issues (information about a fact stated at the beginning of a document is lost at some point) and they are inefficient when training as it is hard to parallelize them to use the massive power of GPUs for training. \n",
    "\n",
    "Nowadays the most popular architecture used in NLP tasks is the Transformer. The two problems RNNs had, the Transformer model solves using the <i> attention </i> mechanism which enables to capture dependencies between distant words in text and input sequences can be processed in parallel making the Transformer model highly efficient. We are going to dive more into the architecture of the Transformer when we start to build the model for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import contractions\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# NN importing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Data preprocessing </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>world so cold</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>It starts with pain, followed by hate\\nFueled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>broken</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Freedom!\\nAlone again again alone\\nPatiently w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>3 leaf loser</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>anthem for the underdog</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>adrenaline</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291121</th>\n",
       "      <td>bobby womack</td>\n",
       "      <td>i wish he didn t trust me so much</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>en</td>\n",
       "      <td>I'm the best friend he's got I'd give him the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291122</th>\n",
       "      <td>bad boys blue</td>\n",
       "      <td>i totally miss you</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Bad Boys Blue \"I Totally Miss You\" I did you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291123</th>\n",
       "      <td>celine dion</td>\n",
       "      <td>sorry for love</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Forgive me for the things That I never said to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291124</th>\n",
       "      <td>dan bern</td>\n",
       "      <td>cure for aids</td>\n",
       "      <td>Indie</td>\n",
       "      <td>en</td>\n",
       "      <td>The day they found a cure for AIDS The day the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291125</th>\n",
       "      <td>crawdad republic</td>\n",
       "      <td>iceberg meadows</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Fourth of July has come, it's custom that we g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291126 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Artist                               Song  Genre Language   \n",
       "0              12 stones                      world so cold   Rock       en  \\\n",
       "1              12 stones                             broken   Rock       en   \n",
       "2              12 stones                       3 leaf loser   Rock       en   \n",
       "3              12 stones            anthem for the underdog   Rock       en   \n",
       "4              12 stones                         adrenaline   Rock       en   \n",
       "...                  ...                                ...    ...      ...   \n",
       "291121      bobby womack  i wish he didn t trust me so much    R&B       en   \n",
       "291122     bad boys blue                 i totally miss you    Pop       en   \n",
       "291123       celine dion                     sorry for love    Pop       en   \n",
       "291124          dan bern                      cure for aids  Indie       en   \n",
       "291125  crawdad republic                    iceberg meadows    Pop       en   \n",
       "\n",
       "                                                   Lyrics  \n",
       "0       It starts with pain, followed by hate\\nFueled ...  \n",
       "1       Freedom!\\nAlone again again alone\\nPatiently w...  \n",
       "2       Biting the hand that feeds you, lying to the v...  \n",
       "3       You say you know just who I am\\nBut you can't ...  \n",
       "4       My heart is beating faster can't control these...  \n",
       "...                                                   ...  \n",
       "291121  I'm the best friend he's got I'd give him the ...  \n",
       "291122  Bad Boys Blue \"I Totally Miss You\" I did you w...  \n",
       "291123  Forgive me for the things That I never said to...  \n",
       "291124  The day they found a cure for AIDS The day the...  \n",
       "291125  Fourth of July has come, it's custom that we g...  \n",
       "\n",
       "[291126 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only work on songs in English, so we will keep only samples in English. After this step we can also drop the language columns as it not necessary anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df_train = df_train[df_train[\"Language\"] == 'en'] # removing language\n",
    "df_train = en_df_train.drop(columns='Language') # language column now more needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get the unique genres in the genre columns. There will be some noisy data, so we will filter it and jest keep the genres that make sense. We will construct a dictionary where the keys will be the genres and the values are the dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rock' 'Metal' 'Pop' 'Indie' 'Folk' 'Electronic' 'R&B' 'Jazz' 'Hip-Hop'\n",
      " 'Country']\n",
      "Number of training samples: 242027\n",
      "Number of test samples: 7440\n"
     ]
    }
   ],
   "source": [
    "genres = df_train['Genre'].unique() # there is some noisy data\n",
    "print(genres)\n",
    "genres = ['Rock', 'Metal', 'Pop', 'Indie', 'R&B', 'Electronic', 'Jazz', 'Hip-Hop', 'Country'] # the genres that we will keep\n",
    "\n",
    "data_train = {} # dictionary of genres used as keys\n",
    "data_test = {}\n",
    "train_samples = 0 # number of samples\n",
    "test_samples = 0\n",
    "for g in genres:\n",
    "    data_train[g] = df_train[df_train[\"Genre\"] == g]\n",
    "    data_test[g] = df_test[df_test[\"Genre\"] == g]\n",
    "    train_samples += len(data_train[g])\n",
    "    test_samples += len(data_test[g])\n",
    "print(\"Number of training samples: \" + str(train_samples))\n",
    "print(\"Number of test samples: \" + str(test_samples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect the obtained data. For training purposes we should have balanced data across the different classes. So, let us see how classes compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music genre: Rock. Number of samples: 107145. Percentage of dataset: 44.2698541898218.\n",
      "\n",
      "Music genre: Metal. Number of samples: 19133. Percentage of dataset: 7.905316349002384.\n",
      "\n",
      "Music genre: Pop. Number of samples: 86297. Percentage of dataset: 35.655939213393545.\n",
      "\n",
      "Music genre: Indie. Number of samples: 7240. Percentage of dataset: 2.9914017857511763.\n",
      "\n",
      "Music genre: R&B. Number of samples: 2765. Percentage of dataset: 1.1424345217682326.\n",
      "\n",
      "Music genre: Electronic. Number of samples: 2005. Percentage of dataset: 0.8284199696728051.\n",
      "\n",
      "Music genre: Jazz. Number of samples: 13314. Percentage of dataset: 5.50103914026121.\n",
      "\n",
      "Music genre: Hip-Hop. Number of samples: 2238. Percentage of dataset: 0.9246902205125874.\n",
      "\n",
      "Music genre: Country. Number of samples: 1890. Percentage of dataset: 0.7809046098162602.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data(data, n_samples):\n",
    "    for key in data.keys():\n",
    "        print(\"Music genre: {}. Number of samples: {}. Percentage of dataset: {}.\\n\".format(key, len(data[key]), 100*len(data[key])/n_samples))\n",
    "print_data(data_train, train_samples)\n",
    "# print_data(data_test, test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is unbalanced to a great extent. Rock and Pop are the most present genres whereas Country and Electronic music are the least present. To keep data at a large enough level, we will drop the Country and Electronic genres and downsize other genres to about $2500$ random samples to keep the dataset balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the two least present genres\n",
    "del data_train['Electronic']\n",
    "del data_train['Country']\n",
    "del data_test['Electronic']\n",
    "del data_test['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new number of samples after deleting Electronic and Country genres\n",
    "train_samples = 0\n",
    "test_samples = 0\n",
    "for key in data_train.keys(): train_samples+=len(data_train[key])\n",
    "for key in data_test.keys(): test_samples+=len(data_test[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music genre: Rock. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: Metal. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: Pop. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: Indie. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: R&B. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: Jazz. Number of samples: 2500. Percentage of dataset: 14.5028425571412.\n",
      "\n",
      "Music genre: Hip-Hop. Number of samples: 2238. Percentage of dataset: 12.982944657152801.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genres = ['Rock', 'Metal', 'Pop', 'Indie', 'R&B', 'Jazz', 'Hip-Hop'] # the genres that we will keep\n",
    "train_samples = 0\n",
    "for g in genres:\n",
    "    if g == 'Hip-Hop': \n",
    "        train_samples += len(data_train[g])\n",
    "        continue\n",
    "    data_train[g] = data_train[g].sample(n=2500, random_state=0)\n",
    "    train_samples += len(data_train[g])\n",
    "print_data(data_train, train_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a decently balanced dataset, so we can proceed with some text preprocessing. Let us look at the lyrics of the first rock song to get the idea of the text we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was my lover\n",
      "She was working undercover\n",
      "Oh the woman knew all of the moves\n",
      "She really had me rompin'\n",
      "We were barefoot stompin'\n",
      "She just kept igniting my fuse\n",
      "\n",
      "I was blinded by the blackness\n",
      "Of her long silk stockings\n",
      "She was rocking with an optical illusion\n",
      "This ain't how I'd thought it'd be\n",
      "She just kept on keeping me\n",
      "In a total state of confusion\n",
      "\n",
      "She took me for a ride\n",
      "Rattled me down to my shoes\n",
      "And I found out\n",
      "She was an undercover agent for the blues\n",
      "\n",
      "She never really needed love\n",
      "Omnidirectional\n",
      "I was just an innocent bystander\n",
      "She kept on getting kinkier\n",
      "I sank hook, line, and sinker\n",
      "Just, just, just too hot to handle\n",
      "\n",
      "She took me by storm\n",
      "It must of been a season for the fools\n",
      "She's so bad\n",
      "An undercover agent for the blues\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_train['Rock']['Lyrics'].iloc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now normalize the text. We are going to do the following:\n",
    "<br>\n",
    "- Tokenize the text\n",
    "- Expand token contractions ('cause $\\rightarrow$ because)\n",
    "- Convert all characters to lowercase\n",
    "- Remove punctuation signs\n",
    "</br>\n",
    "\n",
    "We won't do word lemmatization or stop-word removal as our model of choice is the Transformer which benefits from both, fully expanded tokens and also stop-words as they give context to text. Before text normalization we will split the lyrics into verses, and then the text normalization will be done on the verses, which implies that we will be doing <i> sentence-level </i> instead of <i> token-level </i> classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_verse(verse):\n",
    "  expanded_words = []   \n",
    "  for word in verse.split():\n",
    "    # using contractions to expand the shortened words\n",
    "    expanded_words.append(contractions.fix(word))  \n",
    "    \n",
    "  expanded_lyrics = ' '.join(expanded_words)\n",
    "  expanded_lyrics = re.sub(r\"in'\", \"ing\", expanded_lyrics) # taking into account verbs that end in \"in'\", singin' -> singing\n",
    "\n",
    "  tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # remove punctuations and other non-alphanumeric characters\n",
    "  tokens = tokenizer.tokenize(expanded_lyrics)\n",
    "\n",
    "  tokens = [token.lower() for token in tokens] # lower tokens\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['she', 'was', 'my', 'lover'], ['she', 'was', 'working', 'undercover'], ['oh', 'the', 'woman', 'knew', 'all', 'of', 'the', 'moves'], ['she', 'really', 'had', 'me', 'romping'], ['we', 'were', 'barefoot', 'stomping'], ['she', 'just', 'kept', 'igniting', 'my', 'fuse'], ['i', 'was', 'blinded', 'by', 'the', 'blackness'], ['of', 'her', 'long', 'silk', 'stockings'], ['she', 'was', 'rocking', 'with', 'an', 'optical', 'illusion'], ['this', 'are', 'not', 'how', 'i', 'would', 'thought', 'it', 'would', 'be'], ['she', 'just', 'kept', 'on', 'keeping', 'me'], ['in', 'a', 'total', 'state', 'of', 'confusion'], ['she', 'took', 'me', 'for', 'a', 'ride'], ['rattled', 'me', 'down', 'to', 'my', 'shoes'], ['and', 'i', 'found', 'out'], ['she', 'was', 'an', 'undercover', 'agent', 'for', 'the', 'blues'], ['she', 'never', 'really', 'needed', 'love'], ['omnidirectional'], ['i', 'was', 'just', 'an', 'innocent', 'bystander'], ['she', 'kept', 'on', 'getting', 'kinkier'], ['i', 'sank', 'hook', 'line', 'and', 'sinker'], ['just', 'just', 'just', 'too', 'hot', 'to', 'handle'], ['she', 'took', 'me', 'by', 'storm'], ['it', 'must', 'of', 'been', 'a', 'season', 'for', 'the', 'fools'], ['she', 'is', 'so', 'bad'], ['an', 'undercover', 'agent', 'for', 'the', 'blues']]\n"
     ]
    }
   ],
   "source": [
    "delimiters = ['.', ';', '\\n']\n",
    "split_pattern = '|'.join(map(re.escape, delimiters))\n",
    "def split_verses(lyrics):\n",
    "    verses = [substring for substring in re.split(split_pattern, lyrics) if substring.strip()]\n",
    "    for i, verse in enumerate(verses):\n",
    "        verses[i] = normalize_verse(verse)\n",
    "    return verses\n",
    "\n",
    "lyrics = data_train['Rock']['Lyrics'].iloc[0]\n",
    "verses = split_verses(lyrics)\n",
    "print(verses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now do this for all samples and all categories. We will also make a new data column \"Tokens\" which will contain the tokenized lyrics to be able to compare the original and tokenized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dict(data):\n",
    "    for key in data.keys():\n",
    "        genre_tokens = []\n",
    "        for i in range(len(data[key])):\n",
    "            genre_tokens.append(split_verses(data[key]['Lyrics'].iloc[i]))\n",
    "        data[key]['Tokens'] = genre_tokens\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12868\\2617988761.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[key]['Tokens'] = genre_tokens\n"
     ]
    }
   ],
   "source": [
    "data_train = tokenize_dict(data_train)\n",
    "data_test = tokenize_dict(data_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of reusing the dataset we will save train and test data so to be able to load it without going through the preprocessing steps in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(data):\n",
    "    dfs = []\n",
    "    for key in data.keys(): dfs.append(data[key])\n",
    "    return pd.concat(dfs)\n",
    "combine_dfs(data_test).to_csv('data/pruned_test.csv', index=False)\n",
    "combine_dfs(data_train).to_csv('data/pruned_train.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Model architecture </h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Address word embeddings\n",
    "- Address transformer architecture\n",
    "- Cross entropy for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Rock', 'Metal', 'Pop', 'Indie', 'R&B', 'Jazz', 'Hip-Hop']\n",
    "n = len(genres)\n",
    "def load_data():\n",
    "    train = pd.read_csv('data/pruned_train.csv')\n",
    "    test = pd.read_csv('data/pruned_test.csv')\n",
    "    for i in range(len(train)):\n",
    "        enc = np.zeros(n)\n",
    "        train.at[i, 'Tokens'] = literal_eval(train.at[i, 'Tokens'])\n",
    "        enc[genres.index(train.at[i, 'Genre'])] = 1.0\n",
    "        train.at[i, 'Genre'] = enc\n",
    "    for i in range(len(test)):\n",
    "        enc = np.zeros(n)\n",
    "        test.at[i, 'Tokens'] = literal_eval(test.at[i, 'Tokens'])\n",
    "        enc[genres.index(test.at[i, 'Genre'])] = 1.0\n",
    "        test.at[i, 'Genre'] = enc\n",
    "    return train, test\n",
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = train['Tokens'].apply(len)\n",
    "max_length_index = max_length.idxmax()\n",
    "max_verse_len = train['Tokens'].apply(lambda x: len(x))\n",
    "\n",
    "max_len = max_length.max()\n",
    "print(\"Maximum lyrics length (in verses):\", max_len)\n",
    "print(\"Index of maximum length:\", max_length_index)\n",
    "\n",
    "print(\"Maximum verse length: \", max_verse_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Mr. Cheeks] Basically, LB Fam to the motherfuckin' death Park side, Queen's niggaz represent Long Isle, how we do? They knew our style Represent niggaz in and out the P now Yo, I could do this mother shit for a while I don't give a fuck, my rap style be true, yo Yo, eh yo, yo, yo, how we do this Hey, yo, well back on my South Side Jamaica part of town Where us real niggas love to get down Where you only hear G and P finessin' tracks up on the tape We stuck in Queens, and I'm not tryin to escape Yo, I'm havin cess', drinkin; I'm kickin raps and Emceein' LB for life, kid, my way of bein' Its time to set up shops; wild in this game and got props And fuck cops; we puffin' lah wit' windows up in drop tops Nothin' stops my crew from gettin' it; we learn from the past Puffin' on this ounce of weed, I got this drink in my glass Conversatin' with myself; what does my future hold? Niggaz is dyin', will I make it past thirty years old? I can't run; I guess I gots to hold it down till I'm done What the fuck's the deal? I been doin' this here from day one Official Queen's nigga; be a Lost Boy till my death Until I breathe my mothafuckin' last breath [Chorus x 2] Eh, yo, from boyz to men We're strictly Fam, no longer friends Let's keep it thorough; I hold it down till it's on again Until we meet again, yo, I'm back up on the street again I'm tryin' to make it; throw out my nine, but pack the heat again [A+] Check this out Yo, yo My mind is reachin' twice that size than it only did last year Three times it's likely to feel clear A+, I transform into a super emcee With super vocals, quicker than Superman can find a phone booth The whole truth, nothin' but the whole truth, I roast you Thermonuclear vocals get hotter than in Shanobal The double O, just abide nuclear explosions Exposin' radiation like a vulcan I'm the only guy that knows why the golden eye Was stolen by five Soviet spies They told me to lie; they don't want to hear the god spit Chop my hands off at the armpits, but I regenerate limbs Like star fish, comin' at you with the hard shit Swallow my beeper and page myself so I can communicate with a dolphin Lyrical arson rush the planet like a million martians committin' arson Walkin' the tarpits in India with snake charmers that place all the weight Down... [Canibus] Yo, A+, fuck the nonsense I got the reinforcements To crush any enemies offense with a hundred thousand horsemen And the hardest muthafucka on the market right here I'll complete in a minute what would take you a light year Extra-terrestrial biological entities with infinite energy Battling for world supremecy Who want to get touched? The Can-i-bus will crush you With hard jigsaw puzzles and strong jaw muscles Ambushin' emcees, jumpin' out the trees Like Vietnamese in fatigues, covered with leaves Interrogatin' you whack emcees like MIB's with dark glasses Askin' you to tell me exactly where that alien craft landed By flashing bright light in your eyes with those silver gamas So when you revive, you can't recall or understand it That's how the Canibus keeps tabs on the planet I use amnesia to neutralize public panic And take advantage of opportunites to do damage I pierce your heart with evil thoughts The only thing faster then tha speed of light is the speed of dark With the jaws of a great white shark, I rip you apart My state-of-the art lyrical lasers is razor sharp Splatter the brain matter of my enemies With the same bullet trajectory that murdered John Kennedy In the back of his cranial cavity, which is actually What happens to any motherfucker for tryin' to battle me [Chorus x 2]\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Lyrics'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17238, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(train['Genre'].to_list())\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, max_sequence_length):\n",
    "    # Inputs\n",
    "    input_ids = Input(shape=(max_sequence_length,), name='input_ids', dtype=tf.int32)\n",
    "    attention_mask = Input(shape=(max_sequence_length,), name='attention_mask', dtype=tf.int32)\n",
    "\n",
    "    # Load pre-trained transformer model\n",
    "    transformer_model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Freeze the transformer layers\n",
    "    transformer_model.trainable = False\n",
    "\n",
    "    # Get the transformer output\n",
    "    transformer_output = transformer_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    # Classification head\n",
    "    output = Dense(num_classes, activation='softmax')(transformer_output[:, 0, :])  # Use the [CLS] token\n",
    "\n",
    "    # Combine inputs and outputs into a Keras model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            5383        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,487,623\n",
      "Trainable params: 5,383\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(7, max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in enumerate(df['Lyrics']):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_len, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_ids = np.zeros((len(train), max_len))\n",
    "X_attn_masks = np.zeros((len(train), max_len))\n",
    "X_input_ids, X_attn_masks = generate_training_data(train, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "dataset = dataset.map(SentimentDatasetMapFunction)\n",
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1/1077 [..............................] - ETA: 20:52:37 - loss: 2.7053 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit(x=dataset, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
